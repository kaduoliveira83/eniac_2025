# -*- coding: utf-8 -*-
"""transformer_artigo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U7zPn1PtIEeWycc4b2ACwcUbDj104hp-
"""

!pip install optuna

from google.colab import drive
drive.mount('/content/drive')

import optuna
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from torch.utils.data import DataLoader, TensorDataset
import time
import os

optuna.logging.set_verbosity(optuna.logging.WARNING)

import warnings
warnings.filterwarnings("ignore", message=".*enable_nested_tensor is True.*")


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

save_path = "/content/drive/MyDrive/RESULTADOS_MODELOS"
os.makedirs(save_path, exist_ok=True)

dados = pd.read_csv('/content/drive/MyDrive/dados_pems/id_21_completo.csv')
dados['ds'] = pd.to_datetime(dados['ds'], errors='coerce')

valores = dados[['delay_60']].values.reshape(-1, 1)
split_1 = int(len(valores) * 0.7)
split_2 = int(len(valores) * 0.85)
valores_treino = valores[:split_1]
valores_validacao = valores[split_1:split_2]
valores_teste = valores[split_2:]

scaler = StandardScaler()
scaler.fit(valores_treino)
valores_treino = scaler.transform(valores_treino)
valores_validacao = scaler.transform(valores_validacao)
valores_teste = scaler.transform(valores_teste)

def gerar_janelas(data, tamanho_janela, horizonte_previsao):
    X, y = [], []
    for i in range(len(data) - tamanho_janela - horizonte_previsao + 1):
        X.append(data[i:i + tamanho_janela])
        y.append(data[i + tamanho_janela:i + tamanho_janela + horizonte_previsao])
    X = np.array(X).reshape(-1, tamanho_janela, 1)
    y = np.array(y).reshape(-1, horizonte_previsao)
    return X, y

# Modelo Transformer
class TransformerModel(nn.Module):
    def __init__(self, input_size, d_model, nhead, num_layers, dim_feedforward, dropout, output_size):
        super().__init__()
        self.embedding = nn.Linear(input_size, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,
                                                   dim_feedforward=dim_feedforward, dropout=dropout,
                                                   batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, output_size)

    def forward(self, x):
        x = self.embedding(x)
        x = self.transformer(x)
        x = x[:, -1, :]  # último tempo
        return self.fc(x)

def run_model(tamanho_janela, horizonte_previsao, X_treino, y_treino, X_val, y_val,
              d_model, nhead, num_layers, dim_feedforward, dropout, lr,
              epochs=100, patience=10):
    model = TransformerModel(1, d_model, nhead, num_layers, dim_feedforward, dropout, horizonte_previsao).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    train_dataset = TensorDataset(torch.Tensor(X_treino), torch.Tensor(y_treino))
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    best_loss = float('inf')
    counter = 0
    for epoch in range(epochs):
        model.train()
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

        model.eval()
        with torch.no_grad():
            val_outputs = model(torch.Tensor(X_val).to(device))
            val_loss = criterion(val_outputs, torch.Tensor(y_val).to(device)).item()

        if val_loss < best_loss:
            best_loss = val_loss
            counter = 0
        else:
            counter += 1
            if counter >= patience:
                break

    return model

def objective(trial):
    d_model = trial.suggest_categorical('d_model', [16, 32, 64])
    nhead = trial.suggest_categorical('nhead', [1, 2, 4])
    num_layers = trial.suggest_int('num_layers', 1, 3)
    dim_feedforward = trial.suggest_int('dim_feedforward', 64, 256)
    dropout = trial.suggest_float('dropout', 0.1, 0.4)
    lr = trial.suggest_float('lr', 1e-5, 1e-3)
    tamanho_janela = trial.suggest_int('tamanho_janela', 1, 48)
    horizonte_previsao = trial.suggest_int('horizonte_previsao', 1, 48)

    X_treino, y_treino = gerar_janelas(valores_treino, tamanho_janela, horizonte_previsao)
    X_val, y_val = gerar_janelas(valores_validacao, tamanho_janela, horizonte_previsao)

    if len(X_treino) == 0 or len(X_val) == 0:
        return float('inf')

    model = run_model(tamanho_janela, horizonte_previsao, X_treino, y_treino, X_val, y_val,
                      d_model, nhead, num_layers, dim_feedforward, dropout, lr)
    model.eval()
    with torch.no_grad():
        y_pred = model(torch.Tensor(X_val).to(device)).cpu().numpy()
        rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    return rmse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)
best_params = study.best_params
print("Melhores hiperparâmetros:", best_params)

pd.DataFrame([best_params]).to_csv(f"{save_path}/melhores_hiperparametros_Transformer.csv", index=False)

treino_valid = np.concatenate((valores_treino, valores_validacao), axis=0)
X_treino_final, y_treino_final = gerar_janelas(treino_valid, best_params['tamanho_janela'], best_params['horizonte_previsao'])
X_teste, y_teste = gerar_janelas(valores_teste, best_params['tamanho_janela'], best_params['horizonte_previsao'])

modelo_final = run_model(
    best_params['tamanho_janela'], best_params['horizonte_previsao'],
    X_treino_final, y_treino_final, X_teste, y_teste,
    best_params['d_model'], best_params['nhead'], best_params['num_layers'],
    best_params['dim_feedforward'], best_params['dropout'], best_params['lr']
)

modelo_final.eval()
with torch.no_grad():
    y_pred = modelo_final(torch.Tensor(X_teste).to(device)).cpu().numpy()
    rmse_final = np.sqrt(mean_squared_error(y_teste, y_pred))
print(f"RMSE final no conjunto de teste: {rmse_final:.4f}")

torch.save(modelo_final.state_dict(), f"{save_path}/modelo_Transformer_final.pth")

resultados = []
for tamanho_janela in range(1, 49):
    for horizonte_previsao in range(1, 49):
        X_treino, y_treino = gerar_janelas(treino_valid, tamanho_janela, horizonte_previsao)
        X_teste, y_teste = gerar_janelas(valores_teste, tamanho_janela, horizonte_previsao)
        if len(X_treino) == 0 or len(X_teste) == 0:
            continue
        modelo = run_model(tamanho_janela, horizonte_previsao, X_treino, y_treino, X_teste, y_teste,
                           best_params['d_model'], best_params['nhead'], best_params['num_layers'],
                           best_params['dim_feedforward'], best_params['dropout'], best_params['lr'])
        modelo.eval()
        with torch.no_grad():
            y_pred = modelo(torch.Tensor(X_teste).to(device)).cpu().numpy()
            rmse = np.sqrt(mean_squared_error(y_teste, y_pred))
        resultados.append({
            'tamanho_janela': tamanho_janela,
            'horizonte_previsao': horizonte_previsao,
            'rmse': rmse
        })
        pd.DataFrame(resultados).to_csv(f"{save_path}/grid_rmse_Transformer_parcial.csv", index=False)

pd.DataFrame(resultados).to_csv(f"{save_path}/grid_rmse_Transformer.csv", index=False)
print(f"Resultados salvos em '{save_path}/grid_rmse_Transformer.csv'")